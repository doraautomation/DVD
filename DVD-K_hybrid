import hashlib
import datetime as date
import json
import pandas as pd
import numpy as np
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error
from mpi4py import MPI
from blspy import PrivateKey, AugSchemeMPL, G1Element, G2Element
import sys
import os
import threading
import time

os.makedirs('output', exist_ok=True)

class Block:
    def __init__(self, index, timestamp, data, previous_hash):
        self.index = index
        self.timestamp = timestamp
        self.data = data
        self.previous_hash = previous_hash
        self.hash = self.calculate_hash()

    def calculate_hash(self):
        hash_string = str(self.index) + str(self.timestamp) + str(self.data) + str(self.previous_hash)
        return hashlib.sha256(hash_string.encode()).hexdigest()

class Blockchain:
    def __init__(self):
        self.chain = [self.create_genesis_block()]

    def create_genesis_block(self):
        return Block(0, date.datetime.now(), "Genesis Block", "0")

    def get_latest_block(self):
        return self.chain[-1]

    def add_block(self, new_block):
        new_block.previous_hash = self.get_latest_block().hash
        new_block.hash = new_block.calculate_hash()
        self.chain.append(new_block)

    def is_valid(self):
        for i in range(1, len(self.chain)):
            current = self.chain[i]
            prev = self.chain[i - 1]
            if current.hash != current.calculate_hash():
                return False
            if current.previous_hash != prev.hash:
                return False
        return True

    def consensus(self, block, rank, sub_cluster_size=20):
        block_hash = hashlib.sha256(block.data.encode()).digest()
        signatures = []
        pubkeys = []
        lock = threading.Lock()

        def validator_bls(i):
            seed = hashlib.sha256(f"node-{rank}-thread-{i}".encode()).digest()
            sk = AugSchemeMPL.key_gen(seed)
            pk = sk.get_g1()
            if data_validation(block):
                sig = AugSchemeMPL.sign(sk, block_hash)
                with lock:
                    signatures.append(sig)
                    pubkeys.append(pk)

        threads = [threading.Thread(target=validator_bls, args=(i,)) for i in range(sub_cluster_size)]
        for t in threads: t.start()
        for t in threads: t.join()

        if len(signatures) >= int(sub_cluster_size * 0.67):
            agg_sig = AugSchemeMPL.aggregate(signatures)
            if AugSchemeMPL.aggregate_verify(pubkeys, [block_hash] * len(pubkeys), agg_sig):
                self.add_block(block)
                return True

        prepare_results = [False] * sub_cluster_size

        def prepare_phase(i):
            prepare_results[i] = data_validation(block)

        prepare_threads = [threading.Thread(target=prepare_phase, args=(i,)) for i in range(sub_cluster_size)]
        for t in prepare_threads: t.start()
        for t in prepare_threads: t.join()

        if sum(prepare_results) >= int(sub_cluster_size * 0.51):
            def commit_phase(i):
                block_df = pd.DataFrame([{
                    'index': block.index,
                    'timestamp': block.timestamp,
                    'data': block.data,
                    'previous_hash': block.previous_hash,
                    'hash': block.hash
                }])
                block_df.to_csv(f'output/subcluster_node_{i}_coordinator_{rank}.csv', index=False)

            commit_threads = [threading.Thread(target=commit_phase, args=(i,)) for i in range(sub_cluster_size)]
            for t in commit_threads: t.start()
            for t in commit_threads: t.join()

            self.add_block(block)
            return True

        return False

def load_and_preprocess_data(filepath):
    data = pd.read_csv(filepath)
    features = data.drop(['ID', 'Diagnosis'], axis=1, errors='ignore')
    return StandardScaler().fit_transform(features)

def evaluate_pca_quality(data, n_components=17, mse_threshold=0.01):
    pca = PCA(n_components=n_components)
    reduced = pca.fit_transform(data)
    mse = mean_squared_error(data, pca.inverse_transform(reduced))
    return mse < mse_threshold, reduced, mse

def hash_data(data):
    return hashlib.sha256(json.dumps(data).encode()).hexdigest()

def data_validation(block):
    try:
        content = json.loads(block.data)
        return hash_data(content['data']) == content['hash']
    except:
        return False

def distribute_columns(data, comm):
    rank = comm.Get_rank()
    size = comm.Get_size()
    n_cols = data.shape[1]
    per = n_cols // size
    rem = n_cols % size
    start = rank * per + min(rank, rem)
    end = start + per + (1 if rank < rem else 0)
    return data[:, start:end]

def initialize_centroids(data, k):
    indices = np.random.choice(data.shape[0], size=k, replace=False)
    return data[indices]

def assign_clusters(data, centroids):
    distances = np.linalg.norm(data[:, np.newaxis] - centroids, axis=2)
    return np.argmin(distances, axis=1)

def compute_centroids(data, labels, k):
    centroids = np.zeros((k, data.shape[1]))
    for i in range(k):
        cluster = data[labels == i]
        if len(cluster) > 0:
            centroids[i] = np.mean(cluster, axis=0)
    return centroids

def write_blockchain_to_file(blockchain, filename):
    output = []
    for block in blockchain.chain:
        output.append(f"Block #{block.index}\nTimestamp: {block.timestamp}\nData: {block.data}\nHash: {block.hash}\nPrevHash: {block.previous_hash}\n")
    with open(filename, "w") as f:
        f.write("\n".join(output))


def run_kmeans_mode(filepath, k=5, num_steps=100, local_steps=10):
    comm = MPI.COMM_WORLD
    rank = comm.Get_rank()
    size = comm.Get_size()
    start = MPI.Wtime()

    data = load_and_preprocess_data(filepath)

    if rank == 0:
        ok, reduced, mse = evaluate_pca_quality(data)
        if not ok:
            print(f"PCA MSE too high: {mse}")
            sys.exit()
    else:
        reduced = None

    reduced = comm.bcast(reduced, root=0)
    local_data = np.array_split(reduced, size)[rank]
    centroids = initialize_centroids(reduced, k) if rank == 0 else None
    centroids = comm.bcast(centroids, root=0)

    for i in range(num_steps):
        labels = assign_clusters(local_data, centroids)
        local_centroids = compute_centroids(local_data, labels, k)
        global_centroids = np.zeros_like(local_centroids)
        comm.Allreduce(local_centroids, global_centroids, op=MPI.SUM)
        centroids = global_centroids / size

    blockchain = Blockchain()
    blk_data = {
        'coordinator': rank,
        'data': local_data.tolist(),
        'hash': hash_data(local_data.tolist())
    }
    blk = Block(rank + 1, date.datetime.now(), json.dumps(blk_data), "0")
    committed = blockchain.consensus(blk, rank)

    if committed:
        block_df = pd.DataFrame([{
            'index': blk.index,
            'timestamp': blk.timestamp,
            'data': blk.data,
            'previous_hash': blk.previous_hash,
            'hash': blk.hash
        }])
        block_df.to_csv(f'output/kmeans_block_rank_{rank}.csv', index=False)

    end = MPI.Wtime()

    all_blocks = comm.gather(blk if committed else None, root=0)
    if rank == 0:
        for b in all_blocks:
            if b and b.hash not in [blk.hash for blk in blockchain.chain]:
                blockchain.add_block(b)
        write_blockchain_to_file(blockchain, "output/kmeans_blockchain.txt")
        print(f"[KMEANS MODE] Execution Time: {end - start:.4f} sec")


def run_column_shard_mode(filepath):
    comm = MPI.COMM_WORLD
    rank = comm.Get_rank()
    size = comm.Get_size()
    start = MPI.Wtime()

    data = load_and_preprocess_data(filepath)

    if rank == 0:
        ok, reduced, mse = evaluate_pca_quality(data)
        if not ok:
            print(f"PCA MSE too high: {mse}")
            sys.exit()
    else:
        reduced = None

    reduced = comm.bcast(reduced, root=0)
    local_data = distribute_columns(reduced, comm)

    blockchain = Blockchain()
    blk_data = {
        'coordinator': rank,
        'data': local_data.tolist(),
        'hash': hash_data(local_data.tolist())
    }
    blk = Block(rank + 1001, date.datetime.now(), json.dumps(blk_data), "0")
    committed = blockchain.consensus(blk, rank)

    if committed:
        block_df = pd.DataFrame([{
            'index': blk.index,
            'timestamp': blk.timestamp,
            'data': blk.data,
            'previous_hash': blk.previous_hash,
            'hash': blk.hash
        }])
        block_df.to_csv(f'output/column_block_rank_{rank}.csv', index=False)

    end = MPI.Wtime()

    all_blocks = comm.gather(blk if committed else None, root=0)
    if rank == 0:
        for b in all_blocks:
            if b and b.hash not in [blk.hash for blk in blockchain.chain]:
                blockchain.add_block(b)
        write_blockchain_to_file(blockchain, "output/column_blockchain.txt")
        print(f"[COLUMN SHARD MODE] Execution Time: {end - start:.4f} sec")

if __name__ == "__main__":
    filepath = 'wdbc.csv'
    run_kmeans_mode(filepath)
    run_column_shard_mode(filepath)
